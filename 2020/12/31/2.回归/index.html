<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="单变量线性回归模型 线性模型  $$y &#x3D; wx+b+\epsilon$$  损失函数  $$L(w, b)  &#x3D; \sum_{i&#x3D;1}^{m}\left(y_{i}-w x_{i}-b\right)^{2}$$  目标  $$\left(w^{}, b^{}\right) &#x3D; \underset{(w, b)}{\arg \min } \sum_{i&#x3D;1}^{m}\left(y_{i}-w x_">
<meta property="og:type" content="article">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://example.com/2020/12/31/2.%E5%9B%9E%E5%BD%92/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="单变量线性回归模型 线性模型  $$y &#x3D; wx+b+\epsilon$$  损失函数  $$L(w, b)  &#x3D; \sum_{i&#x3D;1}^{m}\left(y_{i}-w x_{i}-b\right)^{2}$$  目标  $$\left(w^{}, b^{}\right) &#x3D; \underset{(w, b)}{\arg \min } \sum_{i&#x3D;1}^{m}\left(y_{i}-w x_">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2020-12-31T06:18:52.244Z">
<meta property="article:modified_time" content="2020-12-31T06:18:52.252Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
<meta name="generator" content="Hexo 5.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-2.回归" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2020/12/31/2.%E5%9B%9E%E5%BD%92/" class="article-date">
  <time class="dt-published" datetime="2020-12-31T06:18:52.244Z" itemprop="datePublished">2020-12-31</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="单变量线性回归"><a href="#单变量线性回归" class="headerlink" title="单变量线性回归"></a><strong>单变量线性回归</strong></h1><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><ul>
<li>线性模型</li>
</ul>
<p>$$<br>y = wx+b+\epsilon<br>$$</p>
<ul>
<li>损失函数</li>
</ul>
<p>$$<br>L(w, b)  = \sum_{i=1}^{m}\left(y_{i}-w x_{i}-b\right)^{2}<br>$$</p>
<ul>
<li>目标</li>
</ul>
<p>$$<br>\left(w^{<em>}, b^{</em>}\right) = \underset{(w, b)}{\arg \min } \sum_{i=1}^{m}\left(y_{i}-w x_{i}-b\right)^{2}<br>$$</p>
<h2 id="最小二乘法"><a href="#最小二乘法" class="headerlink" title="最小二乘法"></a>最小二乘法</h2><p>将 $L(w,b)$ 分别对 $w$ 和 $b$ 求导<br>$$<br>\begin{aligned}<br>\frac{\partial L(w, b)}{\partial w} &amp;=2\left(w \sum_{i=1}^{m} x_{i}^{2}-\sum_{i=1}^{m}\left(y_{i}-b\right) x_{i}\right) \ \frac{\partial L(w, b)}{\partial b} &amp;=2\left(m b-\sum_{i=1}^{m}\left(y_{i}-w x_{i}\right)\right)<br>\end{aligned}<br>$$<br>令 $\frac{\partial L(w, b)}{\partial b} = 0$ ，得<br>$$<br>b=\frac{1}{m} \sum_{i=1}^{m}\left(y_{i}-w x_{i}\right)<br>$$<br>且已知<br>$$<br>\begin{eqnarray}<br>\overline{x} &amp;=&amp;  \frac{1}{m} \sum_{i=1}^{m} x_{i}\<br>\overline{y} &amp;=&amp;  \frac{1}{m} \sum_{i=1}^{m} y_{i}<br>\end{eqnarray}<br>$$<br>可得<br>$$<br>b=\overline{y}-w \overline{x}<br>$$<br>令 $\frac{\partial L(w, b)}{\partial w} = 0$ ，得<br>$$<br>\begin{eqnarray}<br>0 &amp;=&amp; w \sum_{i=1}^{m} x_{i}^{2}-\sum_{i=1}^{m}\left(y_{i}-b\right) x_{i}\<br>w \sum_{i=1}^{m} x_{i}^{2} &amp;=&amp; \sum_{i=1}^{m} y_{i} x_{i}-\sum_{i=1}^{m} b x_{i}<br>\end{eqnarray}<br>$$<br>将 $b$ 代入上式<br>$$<br>\begin{eqnarray}<br>w \sum_{i=1}^{m} x_{i}^{2} &amp;=&amp; \sum_{i=1}^{m} y_{i} x_{i}-\sum_{i=1}^{m}(\overline{y}-w \overline{x}) x_{i} \<br>w \sum_{i=1}^{m} x_{i}^{2} &amp;=&amp; \sum_{i=1}^{m} y_{i} x_{i}-\overline{y} \sum_{i=1}^{m} x_{i}+w \overline{x} \sum_{i=1}^{m} x_{i} \<br>w\left(\sum_{i=1}^{m} x_{i}^{2}-\overline{x} \sum_{i=1}^{m} x_{i}\right) &amp;=&amp;\sum_{i=1}^{m} y_{i} x_{i}-\overline{y} \sum_{i=1}^{m} x_{i} \<br>w &amp;=&amp;\frac{\sum_{i=1}^{m} y_{i} x_{i}-\overline{y} \sum_{i=1}^{m} x_{i}}{\sum_{i=1}^{m} x_{i}^{2}-\overline{x} \sum_{i=1}^{m} x_{i}} \<br>w &amp;=&amp;\frac{\sum_{i=1}^{m} y_{i}\left(x_{i}-\overline{x}\right)}{\sum_{i=1}^{m} x_{i}^{2}-\frac{1}{m}\left(\sum_{i=1}^{m} x_{i}\right)^{2}}<br>\end{eqnarray}<br>$$<br>进一步，将去均值的 $\boldsymbol{x}$ 和 $\boldsymbol{y}$ 分别表示为 $\boldsymbol{x}<em>d$ 和 $\boldsymbol{x}<em>d$，则 $w$ 还可以表示成向量的形式<br>$$<br>\begin{aligned}<br>w &amp;=\frac{\sum</em>{i=1}^{m}\left(y</em>{i} x_{i}-y_{i} \overline{x}-x_{i} \overline{y}+\overline{x} \overline{y}\right)}{\sum_{i=1}^{m}\left(x_{i}^{2}-x_{i} \overline{x}-x_{i} \overline{x}+\overline{x}^{2}\right)} \ &amp;=\frac{\sum_{i=1}^{m}\left(x_{i}-\overline{x}\right)\left(y_{i}-\overline{y}\right)}{\sum_{i=1}^{m}\left(x_{i}-\overline{x}\right)^{2}} \<br>&amp;=\frac{\boldsymbol{x}<em>{d}^{T} \boldsymbol{y}</em>{d}}{\boldsymbol{x}<em>{d}^{T} \boldsymbol{x}</em>{d}}<br>\end{aligned}<br>$$</p>
<h1 id="多元线性回归"><a href="#多元线性回归" class="headerlink" title="多元线性回归"></a><strong>多元线性回归</strong></h1><h2 id="模型-1"><a href="#模型-1" class="headerlink" title="模型"></a>模型</h2><ul>
<li>数据整合</li>
</ul>
<p>合并 $\boldsymbol{w}$ 和 $b$<br>$$<br>\hat{\boldsymbol{w}}=(\boldsymbol{w} ; b)<br>$$</p>
<p>扩展数据集 $D$<br>$$<br>\mathbf{X}=\left(\begin{array}{ccccc}{x_{11}} &amp; {x_{12}} &amp; {\dots} &amp; {x_{1 d}} &amp; {1} \ {x_{21}} &amp; {x_{22}} &amp; {\dots} &amp; {x_{2 d}} &amp; {1} \ {\vdots} &amp; {\vdots} &amp; {\ddots} &amp; {\vdots} &amp; {\vdots} \ {x_{m 1}} &amp; {x_{m 2}} &amp; {\dots} &amp; {x_{m d}} &amp; {1}\end{array}\right)=\left(\begin{array}{cc}{\boldsymbol{x}<em>{1}^{T}} &amp; {1} \ {\boldsymbol{x}</em>{2}^{T}} &amp; {1} \ {\vdots} &amp; {\vdots} \ {\boldsymbol{x}_{m}^{T}} &amp; {1}\end{array}\right)\<br>$$</p>
<ul>
<li>线性模型</li>
</ul>
<p>$$<br>y = \hat{\boldsymbol{w}}^T\boldsymbol{x}+\epsilon<br>$$</p>
<ul>
<li>损失函数</li>
</ul>
<p>$$<br>\begin{eqnarray}<br>L(\hat{\boldsymbol{w}})<br>&amp;=&amp; \sum_{i=1}^{m}(y_i-\hat{\boldsymbol{w}}^T\boldsymbol{x}<em>i)^2 \<br>&amp;=&amp; |\boldsymbol{y}-\mathbf{X} \hat{\boldsymbol{w}}|</em>{2}^{2} \<br>&amp;=&amp; (\boldsymbol{y}-\mathbf{X} \hat{\boldsymbol{w}})^{T}(\boldsymbol{y}-\mathbf{X} \hat{\boldsymbol{w}})<br>\end{eqnarray}<br>$$</p>
<ul>
<li>目标</li>
</ul>
<p>$$<br>\hat{\boldsymbol{w}}^{*}=\underset{\hat{\boldsymbol{w}}}{\arg \min }(\boldsymbol{y}-\mathbf{X} \hat{\boldsymbol{w}})^{T}(\boldsymbol{y}-\mathbf{X} \hat{\boldsymbol{w}})<br>$$</p>
<h2 id="最小二乘法-1"><a href="#最小二乘法-1" class="headerlink" title="最小二乘法"></a>最小二乘法</h2><p>将 $L(\hat{\boldsymbol{w}})$ 展开<br>$$<br>\begin{eqnarray}<br>L(\hat{\boldsymbol{w}})<br>&amp;=&amp; (\boldsymbol{y}-\hat{\boldsymbol{w}}^T\mathbf{X}^T )(\boldsymbol{y}-\mathbf{X} \hat{\boldsymbol{w}})\<br>&amp;=&amp;\boldsymbol{y}^{T} \boldsymbol{y}-\boldsymbol{y}^{T} \mathbf{X} \hat{\boldsymbol{w}}-\hat{\boldsymbol{w}}^{T} \mathbf{X}^{T} \boldsymbol{y}+\hat{\boldsymbol{w}}^{T} \mathbf{X}^{T} \mathbf{X} \hat{\boldsymbol{w}}<br>\end{eqnarray}<br>$$<br>将 $L(\hat{\boldsymbol{w}})$ 对 $\hat{\boldsymbol{w}}$ 求导<br>$$<br>\begin{eqnarray}<br>\frac{\partial L(\hat{\boldsymbol{w}})}{\partial \hat{\boldsymbol{w}}}<br>&amp;=&amp; \frac{\partial \boldsymbol{y}^{T} \boldsymbol{y}}{\partial \hat{\boldsymbol{w}}}-\frac{\partial \boldsymbol{y}^{T} \mathbf{X} \hat{\boldsymbol{w}}}{\partial \hat{\boldsymbol{w}}}-\frac{\partial \hat{\boldsymbol{w}}^{T} \mathbf{X}^{T} \boldsymbol{y}}{\partial \hat{\boldsymbol{w}}}+\frac{\partial \hat{\boldsymbol{w}}^{T} \mathbf{X}^{T} \mathbf{X} \hat{\boldsymbol{w}}}{\partial \hat{\boldsymbol{w}}}\<br>&amp;=&amp; 0-\mathbf{X}^{T} \boldsymbol{y}-\mathbf{X}^{T} \boldsymbol{y}+\left(\mathbf{X}^{T} \mathbf{X}+\mathbf{X}^{T} \mathbf{X}\right) \hat{w}\<br>&amp;=&amp; 2\mathbf{X}^{T}(\mathbf{X} \hat{\boldsymbol{w}}-\boldsymbol{y})<br>\end{eqnarray}<br>$$<br>① 当 $\mathbf{X}^{T} \mathbf{X}$ 满秩时，令 $\frac{\partial L(\hat{\boldsymbol{w}})}{\partial \hat{\boldsymbol{w}}} = 0$ ，得<br>$$<br>\hat{\boldsymbol{w}}^{*}=\left(\mathbf{X}^{T} \mathbf{X}\right)^{-1} \mathbf{X}^{T} \boldsymbol{y}<br>$$</p>
<p>② 当 $\mathbf{X}^{T} \mathbf{X}$ 不满秩时，加入 L2​ 正则项，即 Ridge 回归。</p>
<h2 id="最大似然法"><a href="#最大似然法" class="headerlink" title="最大似然法"></a>最大似然法</h2><ul>
<li>目标分布</li>
</ul>
<p>$$<br>P\left(y_{i} | \boldsymbol{x}, \hat{\boldsymbol{w}}\right)=\frac{1}{\sqrt{2 \pi} \sigma} e^{-\frac{\left(y_{i}-\sum_{j=1}^{n+1} x_{i j} w_{j}\right)^{2}}{2 \sigma^{2}}}<br>$$</p>
<ul>
<li>似然函数</li>
</ul>
<p>$$<br>\begin{eqnarray}<br>L(\hat{\boldsymbol{w}})<br>&amp;=&amp; \log \prod_{i=1}^{m} P\left(y_{i} | \boldsymbol{x}, \hat{\boldsymbol{w}}\right)\<br>&amp;=&amp; \prod_{i=1}^{m} \log P\left(y_{i} | \boldsymbol{x}, \hat{\boldsymbol{w}}\right)\<br>&amp;=&amp; \sum_{i=1}^{m} \log \frac{1}{\sqrt{2 \pi} \sigma} e^{\frac{\left(y_{i}-\sum_{j=1}^{n+1} x_{i j} w_{j}\right)^{2}}{2 \sigma^{2}}}\<br>&amp;=&amp; m \log \frac{1}{\sqrt{2 \pi} \sigma}-\frac{1}{2 \sigma^{2}} \sum_{i=1}^{m}\left(y_{i}-\sum_{j=1}^{n+1} x_{i j} w_{j}\right)^{2}<br>\end{eqnarray}<br>$$</p>
<p>令 $\frac{\partial L(\hat{\boldsymbol{w}})}{\partial \hat{\boldsymbol{w}}} = 0$ ，得<br>$$<br>\hat{\boldsymbol{w}}^{*}=\left(\mathbf{X}^{T} \mathbf{X}\right)^{-1} \mathbf{X}^{T} \boldsymbol{y}<br>$$</p>
<h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> r2_score</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line"><span class="comment">#线性回归</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyLinearRegression</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.beta = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span>(<span class="params">self,data,labels</span>):</span></span><br><span class="line">        m,n = np.shape(data)</span><br><span class="line">        data = np.column_stack((data,np.ones(m)))</span><br><span class="line">        labels = np.matrix(labels).T</span><br><span class="line">        xTx = np.dot(data.T,data)</span><br><span class="line">        <span class="keyword">if</span> np.linalg.det(xTx) == <span class="number">0.0</span>:</span><br><span class="line">            print(<span class="string">&quot;矩阵为奇异矩阵,不能求逆&quot;</span>)</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        self.beta = np.dot(xTx.I,np.dot(data.T,labels))</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span>(<span class="params">self,data</span>):</span></span><br><span class="line">        m,n = np.shape(data)</span><br><span class="line">        data = np.column_stack((data,np.ones(m)))</span><br><span class="line">        <span class="keyword">return</span> data*self.beta</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">score</span>(<span class="params">self,data,labels</span>):</span></span><br><span class="line">        <span class="keyword">return</span> r2_score(labels,self.predict(data))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    data,labels = datasets.make_friedman1(<span class="number">1000</span>,noise=<span class="number">0.2</span>)</span><br><span class="line">    train_data,test_data,train_labels,test_labels = train_test_split(data,labels,test_size=<span class="number">0.3</span>,random_state=<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#线性回归    </span></span><br><span class="line">    clf = MyLinearRegression()</span><br><span class="line">    clf.fit(train_data,train_labels)</span><br><span class="line">    print(<span class="string">&#x27;训练集&#x27;</span>,clf.score(train_data,train_labels))</span><br><span class="line">    print(<span class="string">&#x27;测试集&#x27;</span>,clf.score(test_data,test_labels))</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#sklearn</span></span><br><span class="line">    clf = LinearRegression()</span><br><span class="line">    clf.fit(train_data,train_labels)</span><br><span class="line">    print(<span class="string">&#x27;训练集&#x27;</span>,clf.score(train_data,train_labels))</span><br><span class="line">    print(<span class="string">&#x27;测试集&#x27;</span>,clf.score(test_data,test_labels))</span><br><span class="line">    </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
<h2 id="Ridge"><a href="#Ridge" class="headerlink" title="Ridge"></a>Ridge</h2><ul>
<li>目的</li>
</ul>
<p>解决 $\mathbf{X}^{T} \mathbf{X}$ 满秩时，无法求逆的问题</p>
<ul>
<li>损失函数</li>
</ul>
<p>$$<br>\begin{eqnarray}<br>L(\hat{\boldsymbol{w}})<br>&amp;=&amp; \sum_{i=1}^{m}\left[(y_i-\hat{\boldsymbol{w}}^T\boldsymbol{x}<em>i)^2+\lambda|\hat{\boldsymbol{w}}|</em>{2}^{2}\right] \<br>&amp;=&amp; |\boldsymbol{y}-\mathbf{X} \hat{\boldsymbol{w}}|<em>{2}^{2}+\lambda|\hat{\boldsymbol{w}}|</em>{2}^{2} \<br>&amp;=&amp; (\boldsymbol{y}-\mathbf{X} \hat{\boldsymbol{w}})^{T}(\boldsymbol{y}-\mathbf{X} \hat{\boldsymbol{w}})+\lambda\hat{\boldsymbol{w}}^T\hat{\boldsymbol{w}}<br>\end{eqnarray}<br>$$</p>
<ul>
<li>目标</li>
</ul>
<p>$$<br>\hat{\boldsymbol{w}}^{*} = \underset{\hat{\boldsymbol{w}}}{\arg \min }\left[(\boldsymbol{y}-\mathbf{X} \hat{\boldsymbol{w}})^{T}(\boldsymbol{y}-\mathbf{X} \hat{\boldsymbol{w}})+\lambda\hat{\boldsymbol{w}}^T\hat{\boldsymbol{w}}\right]<br>$$</p>
<ul>
<li>求解</li>
</ul>
<p>将 $L(\hat{\boldsymbol{w}})$ 展开<br>$$<br>\begin{eqnarray}<br>L(\hat{\boldsymbol{w}})<br>&amp;=&amp; (\boldsymbol{y}-\hat{\boldsymbol{w}}^T\mathbf{X}^T )(\boldsymbol{y}-\mathbf{X} \hat{\boldsymbol{w}})+\lambda\hat{\boldsymbol{w}}^T\hat{\boldsymbol{w}}\<br>&amp;=&amp;\boldsymbol{y}^{T} \boldsymbol{y}-\boldsymbol{y}^{T} \mathbf{X} \hat{\boldsymbol{w}}-\hat{\boldsymbol{w}}^{T} \mathbf{X}^{T} \boldsymbol{y}+\hat{\boldsymbol{w}}^{T} \mathbf{X}^{T} \mathbf{X} \hat{\boldsymbol{w}}+\lambda\hat{\boldsymbol{w}}^T\hat{\boldsymbol{w}}<br>\end{eqnarray}<br>$$<br>将 $L(\hat{\boldsymbol{w}})$ 对 $\hat{\boldsymbol{w}}$ 求导<br>$$<br>\begin{eqnarray}\frac{\partial L(\hat{\boldsymbol{w}})}{\partial\hat{\boldsymbol{w}}}<br>&amp;=&amp; \frac{\partial \boldsymbol{y}^{T} \boldsymbol{y}}{\partial\hat{\boldsymbol{w}}}-\frac{\partial \boldsymbol{y}^{T} \mathbf{X}\hat{\boldsymbol{w}}}{\partial \hat{\boldsymbol{w}}}-\frac{\partial\hat{\boldsymbol{w}}^{T} \mathbf{X}^{T} \boldsymbol{y}}{\partial\hat{\boldsymbol{w}}}+\frac{\partial \hat{\boldsymbol{w}}^{T} \mathbf{X}^{T} \mathbf{X} \hat{\boldsymbol{w}}}{\partial\hat{\boldsymbol{w}}}+\frac{\partial\lambda\hat{\boldsymbol{w}}^T\hat{\boldsymbol{w}}}{\partial\hat{\boldsymbol{w}}}\<br>&amp;=&amp; 0-\mathbf{X}^{T} \boldsymbol{y}-\mathbf{X}^{T} \boldsymbol{y}+\left(\mathbf{X}^{T} \mathbf{X}+\mathbf{X}^{T} \mathbf{X}\right) \hat{w}+\lambda\hat{\boldsymbol{w}}+\lambda\hat{\boldsymbol{w}}\<br>&amp;=&amp; 2\mathbf{X}^{T}(\mathbf{X} \hat{\boldsymbol{w}}-\boldsymbol{y})+2+\lambda\hat{\boldsymbol{w}}\end{eqnarray}<br>$$<br>令 $\frac{\partial L(\hat{\boldsymbol{w}})}{\partial \hat{\boldsymbol{w}}} = 0$ ，得<br>$$<br>\hat{\boldsymbol{w}}^{*}=\left(\mathbf{X}^{T} \mathbf{X}+\lambda\mathbf{I}\right)^{-1} \mathbf{X}^{T} \boldsymbol{y}<br>$$</p>
<h2 id="Lasso"><a href="#Lasso" class="headerlink" title="Lasso"></a>Lasso</h2><ul>
<li>目的</li>
</ul>
<p>保留重要变量</p>
<ul>
<li>损失函数</li>
</ul>
<p>$$<br>\begin{eqnarray}<br>L(\hat{\boldsymbol{w}})<br>&amp;=&amp; \sum_{i=1}^{m}\left[(y_i-\hat{\boldsymbol{w}}^T\boldsymbol{x}<em>i)^2+\lambda|\hat{\boldsymbol{w}}|</em>{1}\right] \<br>&amp;=&amp; |\boldsymbol{y}-\mathbf{X} \hat{\boldsymbol{w}}|<em>{2}^{2}+\lambda|\hat{\boldsymbol{w}}|</em>{1}\<br>&amp;=&amp; (\boldsymbol{y}-\mathbf{X} \hat{\boldsymbol{w}})^{T}(\boldsymbol{y}-\mathbf{X} \hat{\boldsymbol{w}})+\lambda|\hat{\boldsymbol{w}}|_{1}<br>\end{eqnarray}<br>$$</p>
<ul>
<li>目标</li>
</ul>
<p>$$<br>\hat{\boldsymbol{w}}^{*} = \underset{\hat{\boldsymbol{w}}}{\arg \min }\left[(\boldsymbol{y}-\mathbf{X} \hat{\boldsymbol{w}})^{T}(\boldsymbol{y}-\mathbf{X} \hat{\boldsymbol{w}})+\lambda|\hat{\boldsymbol{w}}|_{1}\right]<br>$$</p>
<ul>
<li>求解</li>
</ul>
<p>① GD</p>
<p>② ISTA</p>
<p>③ FISTA</p>
<h1 id="非线性回归"><a href="#非线性回归" class="headerlink" title="非线性回归"></a><strong>非线性回归</strong></h1><h2 id="Spline"><a href="#Spline" class="headerlink" title="Spline"></a>Spline</h2><h2 id="RBF"><a href="#RBF" class="headerlink" title="RBF"></a>RBF</h2>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2020/12/31/2.%E5%9B%9E%E5%BD%92/" data-id="ckjcglc0a0000wodogri7bjkv" data-title="" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2020/12/31/1.%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AF%84%E4%BC%B0%E4%B8%8E%E9%80%89%E6%8B%A9/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          (no title)
        
      </div>
    </a>
  
  
    <a href="/2020/12/31/zhuzhuzhu/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">zhuzhuzhu</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/12/">December 2020</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2020/12/31/1.%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AF%84%E4%BC%B0%E4%B8%8E%E9%80%89%E6%8B%A9/">(no title)</a>
          </li>
        
          <li>
            <a href="/2020/12/31/2.%E5%9B%9E%E5%BD%92/">(no title)</a>
          </li>
        
          <li>
            <a href="/2020/12/31/zhuzhuzhu/">zhuzhuzhu</a>
          </li>
        
          <li>
            <a href="/2020/12/31/hello-world/">Hello World</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2020 John Doe<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.4.1.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>